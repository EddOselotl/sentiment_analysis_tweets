{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelado_ejemplo_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "itam_intro_to_ds",
      "language": "python",
      "name": "itam_intro_to_ds"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElenaVillano/sentiment_analysis_tweets/blob/main/notebooks/modelado_ejemplo_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrmrcgOf0Wd_"
      },
      "source": [
        "!pip install \"git+https://github.com/ElenaVillano/sentiment_analysis_tweets.git#egg=nlptweet&subdirectory=src\" --quiet"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg5YRy3E1jMT",
        "outputId": "e0f2c81d-9c9f-4de8-863b-87581c9e2eb3"
      },
      "source": [
        "#!pip uninstall nlptweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling nlptweet-0.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/nlp/*\n",
            "    /usr/local/lib/python3.7/dist-packages/nlptweet-0.1.dist-info/*\n",
            "Proceed (y/n)? yes\n",
            "Your response ('yes') was not one of the expected responses: y, n\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled nlptweet-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpmrJ_G_pgnP",
        "outputId": "42b97d80-1849-476e-a318-0426fc513e57"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import nltk\n",
        "#import re\n",
        "import timeit\n",
        "import string\n",
        "# Nuestro paquete\n",
        "import nlp\n",
        "\n",
        "\n",
        "#call the nltk downloader\n",
        "nltk.download('punkt')\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "# Carga un set de stopwords predefinidas\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP5nN_Ns0WeJ"
      },
      "source": [
        "# Nombramiento de columnas\n",
        "col_names = ['target', # Polaridad del twet 0=negativo, 2=neutral, 4=positivo\n",
        "             'ids',    # ID tweet\n",
        "             'date',   # Fecha y hora del tweet\n",
        "             'flag',   # QUERY\n",
        "             'user',   # Usuario del tweet\n",
        "             'text']   # Texto del tweety"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbcoouZtnEUf"
      },
      "source": [
        "# Carga y limpieza de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "j6uJdbk2nKCX",
        "outputId": "cc690527-27de-4a2d-a3a9-029dbbd10535"
      },
      "source": [
        "# Requiered to select a file to be imported into colab\n",
        "# Not useful if running locally\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d2327d24-9bb6-4ae8-8997-13cadc4c184f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d2327d24-9bb6-4ae8-8997-13cadc4c184f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving smaller_sample_200000.csv to smaller_sample_200000.csv\n",
            "Saving testdata_manual_2009_06_14.csv to testdata_manual_2009_06_14.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5P449bk0WeL"
      },
      "source": [
        "training =  pd.read_csv('smaller_sample_200000.csv',\n",
        "                 encoding='latin-1')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szNOHjbB0WeL"
      },
      "source": [
        "test = pd.read_csv('testdata_manual_2009_06_14.csv', names=col_names)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs77X44P0WeM",
        "outputId": "67812d1f-04a2-4b5f-ecd0-fae8ec907703"
      },
      "source": [
        "print(training.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200000, 6)\n",
            "(498, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "UGANa9pX0WeO",
        "outputId": "274fd650-9b12-419f-cd18-0f2080f73b59"
      },
      "source": [
        "# Ejemplo\n",
        "training.loc[[4,8,27,41,44,35,48,155]]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2250671573</td>\n",
              "      <td>Sat Jun 20 01:09:32 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>evilcaz</td>\n",
              "      <td>Far too tired to be at work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>2233840533</td>\n",
              "      <td>Thu Jun 18 21:59:23 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TiffanyNicoleN</td>\n",
              "      <td>I wish today never happened. On top of that, I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>2254342354</td>\n",
              "      <td>Sat Jun 20 09:36:05 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AJMcCleary</td>\n",
              "      <td>@ReallyShecky - Indeed, come on down. The sick...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4</td>\n",
              "      <td>1794361968</td>\n",
              "      <td>Thu May 14 05:38:02 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>cherrybomb54</td>\n",
              "      <td>Now I get to make incredibly long Tweets thank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>1686244630</td>\n",
              "      <td>Sun May 03 04:31:06 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>anggieholics</td>\n",
              "      <td>btw whats wrong w/ being single? and if i turn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>4</td>\n",
              "      <td>2064953318</td>\n",
              "      <td>Sun Jun 07 07:33:41 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>heidirenee92</td>\n",
              "      <td>@allucha It would be a good idea not to mess w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>4</td>\n",
              "      <td>2004632930</td>\n",
              "      <td>Tue Jun 02 08:30:15 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Hadari_B</td>\n",
              "      <td>At a birthday party of a friend !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>4</td>\n",
              "      <td>1687038778</td>\n",
              "      <td>Sun May 03 07:34:54 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>lynettebrennan</td>\n",
              "      <td>is relaxing on the couch, Sundays are the best</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     target  ...                                               text\n",
              "4         0  ...                       Far too tired to be at work \n",
              "8         0  ...  I wish today never happened. On top of that, I...\n",
              "27        0  ...  @ReallyShecky - Indeed, come on down. The sick...\n",
              "41        4  ...  Now I get to make incredibly long Tweets thank...\n",
              "44        0  ...  btw whats wrong w/ being single? and if i turn...\n",
              "35        4  ...  @allucha It would be a good idea not to mess w...\n",
              "48        4  ...                 At a birthday party of a friend ! \n",
              "155       4  ...    is relaxing on the couch, Sundays are the best \n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0xhfKqV194E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dad07d4-1015-4de4-bf19-b1501a4b1b56"
      },
      "source": [
        "# primera parte limpieza\n",
        "from nlp.preprocessing import convierte_a_minusculas, reemplazar_urls, reemplazar_usuarios, quitar_hashtag\n",
        "# minusculas\n",
        "training = convierte_a_minusculas(training)\n",
        "test = convierte_a_minusculas(test)\n",
        "# url\n",
        "training['text'] = training['text'].map(lambda s: reemplazar_urls(s))\n",
        "test['text'] = test['text'].map(lambda s: reemplazar_urls(s))\n",
        "# user_mention\n",
        "training['text'] = training['text'].map(lambda s: reemplazar_usuarios(s))\n",
        "test['text'] = test['text'].map(lambda s: reemplazar_usuarios(s))\n",
        "# hashtags\n",
        "training['text'] = training['text'].map(lambda s: quitar_hashtag(s))\n",
        "test['text'] = test['text'].map(lambda s: quitar_hashtag(s))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "vDcyf3iNjtcd",
        "outputId": "43c74bab-ad8b-46fe-dedd-7f73a44741ea"
      },
      "source": [
        "# Ejemplo\n",
        "training.loc[[4,8,27,41,44,35,48,155]]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2250671573</td>\n",
              "      <td>sat jun 20 01:09:32 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>evilcaz</td>\n",
              "      <td>far too tired to be at work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>2233840533</td>\n",
              "      <td>thu jun 18 21:59:23 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>tiffanynicolen</td>\n",
              "      <td>i wish today never happened. on top of that, i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>2254342354</td>\n",
              "      <td>sat jun 20 09:36:05 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>ajmccleary</td>\n",
              "      <td>user_mention - indeed, come on down. the sick ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4</td>\n",
              "      <td>1794361968</td>\n",
              "      <td>thu may 14 05:38:02 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>cherrybomb54</td>\n",
              "      <td>now i get to make incredibly long tweets thank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>1686244630</td>\n",
              "      <td>sun may 03 04:31:06 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>anggieholics</td>\n",
              "      <td>btw whats wrong w/ being single? and if i turn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>4</td>\n",
              "      <td>2064953318</td>\n",
              "      <td>sun jun 07 07:33:41 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>heidirenee92</td>\n",
              "      <td>user_mention it would be a good idea not to me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>4</td>\n",
              "      <td>2004632930</td>\n",
              "      <td>tue jun 02 08:30:15 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>hadari_b</td>\n",
              "      <td>at a birthday party of a friend !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>4</td>\n",
              "      <td>1687038778</td>\n",
              "      <td>sun may 03 07:34:54 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>lynettebrennan</td>\n",
              "      <td>is relaxing on the couch, sundays are the best</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     target  ...                                               text\n",
              "4         0  ...                       far too tired to be at work \n",
              "8         0  ...  i wish today never happened. on top of that, i...\n",
              "27        0  ...  user_mention - indeed, come on down. the sick ...\n",
              "41        4  ...  now i get to make incredibly long tweets thank...\n",
              "44        0  ...  btw whats wrong w/ being single? and if i turn...\n",
              "35        4  ...  user_mention it would be a good idea not to me...\n",
              "48        4  ...                 at a birthday party of a friend ! \n",
              "155       4  ...    is relaxing on the couch, sundays are the best \n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6SC37m0sjtW"
      },
      "source": [
        "# segunda parte limpieza\n",
        "from nlp.preprocessing import quitar_RT, quitar_caracteres_especiales, quitar_letras_repetidas\n",
        "# retweets\n",
        "training['text'] = training['text'].map(lambda s: quitar_RT(s))\n",
        "test['text'] = test['text'].map(lambda s: quitar_RT(s))\n",
        "# caracteres especiales\n",
        "training['text'] = training['text'].map(lambda s: quitar_caracteres_especiales(s))\n",
        "test['text'] = test['text'].map(lambda s: quitar_caracteres_especiales(s))\n",
        "# letras repetidas\n",
        "training['text'] = training['text'].map(lambda s: quitar_letras_repetidas(s))\n",
        "test['text'] = test['text'].map(lambda s: quitar_letras_repetidas(s))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "8WwQY-vN2e-t",
        "outputId": "e3b04eda-f55f-4c62-db48-908c74fe05fe"
      },
      "source": [
        "# Ejemplo\n",
        "training.loc[[4,8,27,41,44,35,48,155]]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2250671573</td>\n",
              "      <td>sat jun 20 01:09:32 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>evilcaz</td>\n",
              "      <td>far too tired to be at work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>2233840533</td>\n",
              "      <td>thu jun 18 21:59:23 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>tiffanynicolen</td>\n",
              "      <td>i wish today never happened on top of that i m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>2254342354</td>\n",
              "      <td>sat jun 20 09:36:05 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>ajmccleary</td>\n",
              "      <td>user_mention  indeed come on down the sick hea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4</td>\n",
              "      <td>1794361968</td>\n",
              "      <td>thu may 14 05:38:02 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>cherrybomb54</td>\n",
              "      <td>now i get to make incredibly long tweets thank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>1686244630</td>\n",
              "      <td>sun may 03 04:31:06 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>anggieholics</td>\n",
              "      <td>btw whats wrong w/ being single and if i turn ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>4</td>\n",
              "      <td>2064953318</td>\n",
              "      <td>sun jun 07 07:33:41 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>heidirenee92</td>\n",
              "      <td>user_mention it would be a good idea not to me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>4</td>\n",
              "      <td>2004632930</td>\n",
              "      <td>tue jun 02 08:30:15 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>hadari_b</td>\n",
              "      <td>at a birthday party of a friend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>4</td>\n",
              "      <td>1687038778</td>\n",
              "      <td>sun may 03 07:34:54 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>lynettebrennan</td>\n",
              "      <td>is relaxing on the couch sundays are the best</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     target  ...                                               text\n",
              "4         0  ...                       far too tired to be at work \n",
              "8         0  ...  i wish today never happened on top of that i m...\n",
              "27        0  ...  user_mention  indeed come on down the sick hea...\n",
              "41        4  ...  now i get to make incredibly long tweets thank...\n",
              "44        0  ...  btw whats wrong w/ being single and if i turn ...\n",
              "35        4  ...  user_mention it would be a good idea not to me...\n",
              "48        4  ...                  at a birthday party of a friend  \n",
              "155       4  ...     is relaxing on the couch sundays are the best \n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NLKhB-ikQn9"
      },
      "source": [
        "# tercera parte limpieza\n",
        "from nlp.preprocessing import quitar_nonascii, separar_abreviaciones, remove_stopwords, oracion_raiz\n",
        "#nonascii\n",
        "training['text'] = training['text'].map(lambda s: quitar_nonascii(s))\n",
        "test['text'] = test['text'].map(lambda s: quitar_nonascii(s))\n",
        "# abreviaciones\n",
        "training['text'] = training['text'].map(lambda s: separar_abreviaciones(s))\n",
        "test['text'] = test['text'].map(lambda s: separar_abreviaciones(s))\n",
        "# stop words\n",
        "training['text'] = training['text'].map(lambda s: remove_stopwords(s))\n",
        "test['atext'] = test['text'].map(lambda s: remove_stopwords(s))\n",
        "# raiz\n",
        "training['text'] = training['text'].map(lambda s: oracion_raiz(s))\n",
        "test['atext'] = test['text'].map(lambda s: oracion_raiz(s))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "R_8VxdOJ2c2G",
        "outputId": "386982d8-ed81-4fbb-ad96-57667d583ad5"
      },
      "source": [
        "# Ejemplo\n",
        "training.loc[[4,8,27,41,44,35,48,155]]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2250671573</td>\n",
              "      <td>sat jun 20 01:09:32 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>evilcaz</td>\n",
              "      <td>far tire work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>2233840533</td>\n",
              "      <td>thu jun 18 21:59:23 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>tiffanynicolen</td>\n",
              "      <td>wish today never happen top miss user_ment get...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>2254342354</td>\n",
              "      <td>sat jun 20 09:36:05 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>ajmccleary</td>\n",
              "      <td>user_ment inde come sick heat realli quit refr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4</td>\n",
              "      <td>1794361968</td>\n",
              "      <td>thu may 14 05:38:02 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>cherrybomb54</td>\n",
              "      <td>get make incred long tweet thank twitzer googl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>1686244630</td>\n",
              "      <td>sun may 03 04:31:06 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>anggieholics</td>\n",
              "      <td>btw what wrong w/ singl turn lesbian let u know</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>4</td>\n",
              "      <td>2064953318</td>\n",
              "      <td>sun jun 07 07:33:41 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>heidirenee92</td>\n",
              "      <td>user_ment would good idea mess mr potatoheadju...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>4</td>\n",
              "      <td>2004632930</td>\n",
              "      <td>tue jun 02 08:30:15 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>hadari_b</td>\n",
              "      <td>birthday parti friend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>4</td>\n",
              "      <td>1687038778</td>\n",
              "      <td>sun may 03 07:34:54 pdt 2009</td>\n",
              "      <td>no_query</td>\n",
              "      <td>lynettebrennan</td>\n",
              "      <td>relax couch sunday best</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     target  ...                                               text\n",
              "4         0  ...                                     far tire work \n",
              "8         0  ...  wish today never happen top miss user_ment get...\n",
              "27        0  ...  user_ment inde come sick heat realli quit refr...\n",
              "41        4  ...    get make incred long tweet thank twitzer googl \n",
              "44        0  ...   btw what wrong w/ singl turn lesbian let u know \n",
              "35        4  ...  user_ment would good idea mess mr potatoheadju...\n",
              "48        4  ...                             birthday parti friend \n",
              "155       4  ...                           relax couch sunday best \n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbUJGr8qNjjQ",
        "outputId": "85cb6a2b-a179-48ae-808b-265ac40c8441"
      },
      "source": [
        "training.text"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         user_ment wellnot breakup speech per sebut go ...\n",
              "1                                 state slow seem pick bit \n",
              "2         user_ment hey help user_ment take url behind a...\n",
              "3         aw man user_ment think make see chang thing wo...\n",
              "4                                            far tire work \n",
              "                                ...                        \n",
              "199995       user_ment think kati perri remind situat kirk \n",
              "199996    would bf build someth w/n would much would get...\n",
              "199997    tip finger hurt nervou school store tomorrow p...\n",
              "199998                                    url chillin pool \n",
              "199999                      pick dog doctor note back work \n",
              "Name: text, Length: 200000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgajHM4gCjep"
      },
      "source": [
        "# Entrenamos un Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdTHR74lmbUk"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50TYKCmaEZ_t",
        "outputId": "4e189f33-7d95-4a59-9043-137f1b48b73b"
      },
      "source": [
        "# Entrena un Tokenizer. Consiste en:\n",
        "# Crea un diccionario numerado de las palabras existentes en el corpus, y devuelve\n",
        "# por cada palabra, el número entero de su índice en el diccionario.\n",
        "# En este caso, considera las N palabras más frecuentes.\n",
        "# oov_token = constante asiganda para palabras fuera del vocabulario (NOT USED HERE)\n",
        "data_set_1 = training\n",
        "data_set_1.shape\n",
        "data_set_1.text"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         user_ment wellnot breakup speech per sebut go ...\n",
              "1                                 state slow seem pick bit \n",
              "2         user_ment hey help user_ment take url behind a...\n",
              "3         aw man user_ment think make see chang thing wo...\n",
              "4                                            far tire work \n",
              "                                ...                        \n",
              "199995       user_ment think kati perri remind situat kirk \n",
              "199996    would bf build someth w/n would much would get...\n",
              "199997    tip finger hurt nervou school store tomorrow p...\n",
              "199998                                    url chillin pool \n",
              "199999                      pick dog doctor note back work \n",
              "Name: text, Length: 200000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-hxvgq_AjBL",
        "outputId": "b7c5d51d-e644-4d8a-b094-834471bf86ae"
      },
      "source": [
        "max_words = 5000\n",
        "max_len = 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(data_set_1.text)\n",
        "\n",
        "print(\"Number of words in the dictionary\", len(tokenizer.word_index))\n",
        "print(list(tokenizer.word_index.items())[:10])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the dictionary 81792\n",
            "[('ment', 1), ('user', 2), ('go', 3), ('get', 4), ('day', 5), ('good', 6), ('work', 7), ('like', 8), ('love', 9), ('url', 10)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ejRLzBHE-3S",
        "outputId": "398b3670-83d4-4883-9824-98125a44f775"
      },
      "source": [
        "#sorted(tokenizer.word_counts.values())[:10]\n",
        "#Tokenizer?"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFL2iSx-FHjv",
        "outputId": "c79841d4-c533-469c-baa0-995882902856"
      },
      "source": [
        "# Convierte palabras en enteros\n",
        "train_sequences = tokenizer.texts_to_sequences(data_set_1.text)\n",
        "lengths = []\n",
        "for seq in train_sequences:\n",
        "  lengths.append(len(seq))\n",
        "\n",
        "print(\"Tweet más pequeño\", np.array(lengths).min(), \"words.\")\n",
        "print(\"Tweet más grande\", np.array(lengths).max(), \"words.\")\n",
        "print(f\"Longitud promedio: {np.array(lengths).mean():.2f} +- {np.array(lengths).std():.2f} words.\")\n",
        "print('\\n')\n",
        "\n",
        "for seq in train_sequences[:10]:\n",
        "  print(seq)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweet más pequeño 0 words.\n",
            "Tweet más grande 46 words.\n",
            "Longitud promedio: 7.30 +- 3.74 words.\n",
            "\n",
            "\n",
            "[2, 1, 2053, 1888, 3, 146, 501, 767, 160, 19, 382, 69, 54]\n",
            "[824, 627, 242, 408, 174]\n",
            "[2, 1, 88, 127, 2, 1, 68, 10, 807, 197, 19, 1532, 2, 1, 19, 813]\n",
            "[334, 144, 2, 1, 24, 34, 23, 276, 63, 7, 3]\n",
            "[279, 110, 7]\n",
            "[2, 1, 523, 162, 198, 357]\n",
            "[2, 1, 635, 795, 50, 2228, 64, 837]\n",
            "[2, 1, 284, 46, 200, 352, 482, 874, 1434, 4680, 2137]\n",
            "[50, 12, 99, 195, 480, 15, 2, 1, 4, 325, 67]\n",
            "[4, 138, 3, 18, 18, 3233, 257, 3697, 4941, 287]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8GkE5PeAFfFH",
        "outputId": "cff42aab-72f5-4f7c-e17c-349fdb994b01"
      },
      "source": [
        "plt.hist(lengths,20)\n",
        "plt.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxkMkyh-E1eH",
        "outputId": "81925bb6-c953-4bd7-ab2d-3759c1006794"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(data_set_1.text)\n",
        "tweets = pad_sequences(sequences, maxlen=max_len, padding='pos')\n",
        "print(tweets)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  382   69   54]\n",
            " [   0    0    0 ...  242  408  174]\n",
            " [   0    0    0 ...    1   19  813]\n",
            " ...\n",
            " [   0    0    0 ...   49   81   41]\n",
            " [   0    0    0 ...   10 1071  588]\n",
            " [   0    0    0 ...  765   17    7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm06k8n9E1rd",
        "outputId": "b00b9f84-c584-4d49-c838-e76a02b7f293"
      },
      "source": [
        "tweets.shape"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdbTguzPLK4O",
        "outputId": "02fb77b7-2c47-4e5d-ef79-a5b389588133"
      },
      "source": [
        "data_set_1.target.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY-EM8O70OnW"
      },
      "source": [
        "## Separamos en train, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4-mzuVQm7qH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H29q5XL0RBl"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tweets, data_set_1.target.values, test_size=0.3, random_state=23042021)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gx4QC-MDMRF",
        "outputId": "7538a6ec-ee22-4c37-8afc-e16d3805b31b"
      },
      "source": [
        "print(\"X_train\", X_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print(\"X_test\", X_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (140000, 128)\n",
            "y_train (140000,)\n",
            "X_test (60000, 128)\n",
            "y_test (60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1P3P0d49-ZT"
      },
      "source": [
        "y_train = np.where(y_train==4,1,y_train)\n",
        "y_test = np.where(y_test==4,1,y_test)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QCer5X4-iF9",
        "outputId": "0d53498e-9406-41e2-d50f-26ddb885f316"
      },
      "source": [
        "y_train\n",
        "y_test"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ih2oqSz07em"
      },
      "source": [
        "# Creamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZv_fDCW1Xzt"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l1"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eITD3Xki4Cw5",
        "outputId": "ba482e18-5a38-469b-d400-6c1a76af5822"
      },
      "source": [
        "# Crea una red con layers Embedding, LSTM, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_length=128, input_dim=max_words, output_dim=64, trainable=True))\n",
        "model.add(LSTM(units=64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=82, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1, activation=None))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 128, 64)           320000    \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 82)                5330      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 82)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 83        \n",
            "=================================================================\n",
            "Total params: 358,437\n",
            "Trainable params: 358,437\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U9KxBvV1qun"
      },
      "source": [
        "# Compile\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(), 'accuracy'])"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCPmbUlH1vXP",
        "outputId": "403d8aa2-643c-410a-d2b2-532206d06400"
      },
      "source": [
        "%%timeit\n",
        "\n",
        "# Train it\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_split=0.2)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "7000/7000 [==============================] - 82s 12ms/step - loss: 0.5514 - binary_accuracy: 0.6899 - accuracy: 0.6899 - val_loss: 0.5068 - val_binary_accuracy: 0.7240 - val_accuracy: 0.7240\n",
            "Epoch 2/5\n",
            "7000/7000 [==============================] - 81s 12ms/step - loss: 0.4681 - binary_accuracy: 0.7635 - accuracy: 0.7635 - val_loss: 0.4921 - val_binary_accuracy: 0.7425 - val_accuracy: 0.7425\n",
            "Epoch 3/5\n",
            "7000/7000 [==============================] - 82s 12ms/step - loss: 0.4390 - binary_accuracy: 0.7811 - accuracy: 0.7811 - val_loss: 0.4959 - val_binary_accuracy: 0.7550 - val_accuracy: 0.7550\n",
            "Epoch 4/5\n",
            "7000/7000 [==============================] - 82s 12ms/step - loss: 0.4130 - binary_accuracy: 0.7965 - accuracy: 0.7965 - val_loss: 0.5235 - val_binary_accuracy: 0.7538 - val_accuracy: 0.7538\n",
            "Epoch 5/5\n",
            "7000/7000 [==============================] - 82s 12ms/step - loss: 0.3904 - binary_accuracy: 0.8093 - accuracy: 0.8093 - val_loss: 0.5331 - val_binary_accuracy: 0.7501 - val_accuracy: 0.7501\n",
            "Epoch 1/5\n",
            "7000/7000 [==============================] - 77s 11ms/step - loss: 0.3764 - binary_accuracy: 0.8164 - accuracy: 0.8164 - val_loss: 0.5634 - val_binary_accuracy: 0.7437 - val_accuracy: 0.7437\n",
            "Epoch 2/5\n",
            "7000/7000 [==============================] - 81s 12ms/step - loss: 0.3544 - binary_accuracy: 0.8268 - accuracy: 0.8268 - val_loss: 0.5950 - val_binary_accuracy: 0.7364 - val_accuracy: 0.7364\n",
            "Epoch 3/5\n",
            "7000/7000 [==============================] - 82s 12ms/step - loss: 0.3326 - binary_accuracy: 0.8390 - accuracy: 0.8390 - val_loss: 0.6095 - val_binary_accuracy: 0.7328 - val_accuracy: 0.7328\n",
            "Epoch 4/5\n",
            "7000/7000 [==============================] - 82s 12ms/step - loss: 0.3108 - binary_accuracy: 0.8505 - accuracy: 0.8505 - val_loss: 0.6633 - val_binary_accuracy: 0.7304 - val_accuracy: 0.7304\n",
            "Epoch 5/5\n",
            "7000/7000 [==============================] - 82s 12ms/step - loss: 0.2921 - binary_accuracy: 0.8602 - accuracy: 0.8602 - val_loss: 0.7105 - val_binary_accuracy: 0.7339 - val_accuracy: 0.7339\n",
            "Epoch 1/5\n",
            "7000/7000 [==============================] - 78s 11ms/step - loss: 0.2731 - binary_accuracy: 0.8701 - accuracy: 0.8701 - val_loss: 0.8405 - val_binary_accuracy: 0.7270 - val_accuracy: 0.7270\n",
            "Epoch 2/5\n",
            "7000/7000 [==============================] - 78s 11ms/step - loss: 0.2574 - binary_accuracy: 0.8771 - accuracy: 0.8771 - val_loss: 0.8847 - val_binary_accuracy: 0.7190 - val_accuracy: 0.7190\n",
            "Epoch 3/5\n",
            "7000/7000 [==============================] - 77s 11ms/step - loss: 0.2432 - binary_accuracy: 0.8847 - accuracy: 0.8847 - val_loss: 0.8721 - val_binary_accuracy: 0.7264 - val_accuracy: 0.7264\n",
            "Epoch 4/5\n",
            "7000/7000 [==============================] - 78s 11ms/step - loss: 0.2292 - binary_accuracy: 0.8921 - accuracy: 0.8921 - val_loss: 0.8617 - val_binary_accuracy: 0.7195 - val_accuracy: 0.7195\n",
            "Epoch 5/5\n",
            "7000/7000 [==============================] - 77s 11ms/step - loss: 0.2171 - binary_accuracy: 0.8971 - accuracy: 0.8971 - val_loss: 1.0677 - val_binary_accuracy: 0.7194 - val_accuracy: 0.7194\n",
            "Epoch 1/5\n",
            "7000/7000 [==============================] - 77s 11ms/step - loss: 0.2074 - binary_accuracy: 0.9015 - accuracy: 0.9015 - val_loss: 1.1286 - val_binary_accuracy: 0.7239 - val_accuracy: 0.7239\n",
            "Epoch 2/5\n",
            "7000/7000 [==============================] - 78s 11ms/step - loss: 0.1959 - binary_accuracy: 0.9076 - accuracy: 0.9076 - val_loss: 1.0878 - val_binary_accuracy: 0.7243 - val_accuracy: 0.7243\n",
            "Epoch 3/5\n",
            "7000/7000 [==============================] - 78s 11ms/step - loss: 0.1897 - binary_accuracy: 0.9108 - accuracy: 0.9108 - val_loss: 1.1547 - val_binary_accuracy: 0.7181 - val_accuracy: 0.7181\n",
            "Epoch 4/5\n",
            "7000/7000 [==============================] - 78s 11ms/step - loss: 0.1794 - binary_accuracy: 0.9157 - accuracy: 0.9157 - val_loss: 1.2564 - val_binary_accuracy: 0.7200 - val_accuracy: 0.7200\n",
            "Epoch 5/5\n",
            "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1744 - binary_accuracy: 0.9178 - accuracy: 0.9178 - val_loss: 1.2701 - val_binary_accuracy: 0.7178 - val_accuracy: 0.7178\n",
            "Epoch 1/5\n",
            "7000/7000 [==============================] - 75s 11ms/step - loss: 0.1658 - binary_accuracy: 0.9230 - accuracy: 0.9230 - val_loss: 1.3113 - val_binary_accuracy: 0.7193 - val_accuracy: 0.7193\n",
            "Epoch 2/5\n",
            "7000/7000 [==============================] - 75s 11ms/step - loss: 0.1599 - binary_accuracy: 0.9247 - accuracy: 0.9247 - val_loss: 1.2998 - val_binary_accuracy: 0.7173 - val_accuracy: 0.7173\n",
            "Epoch 3/5\n",
            "7000/7000 [==============================] - 75s 11ms/step - loss: 0.1532 - binary_accuracy: 0.9285 - accuracy: 0.9285 - val_loss: 1.3318 - val_binary_accuracy: 0.7134 - val_accuracy: 0.7134\n",
            "Epoch 4/5\n",
            "7000/7000 [==============================] - 81s 12ms/step - loss: 0.1525 - binary_accuracy: 0.9299 - accuracy: 0.9299 - val_loss: 1.4244 - val_binary_accuracy: 0.7178 - val_accuracy: 0.7178\n",
            "Epoch 5/5\n",
            "7000/7000 [==============================] - 75s 11ms/step - loss: 0.1458 - binary_accuracy: 0.9323 - accuracy: 0.9323 - val_loss: 1.5449 - val_binary_accuracy: 0.7170 - val_accuracy: 0.7170\n",
            "Epoch 1/5\n",
            "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1436 - binary_accuracy: 0.9332 - accuracy: 0.9332 - val_loss: 1.4357 - val_binary_accuracy: 0.7172 - val_accuracy: 0.7172\n",
            "Epoch 2/5\n",
            "7000/7000 [==============================] - 75s 11ms/step - loss: 0.1416 - binary_accuracy: 0.9336 - accuracy: 0.9336 - val_loss: 1.4864 - val_binary_accuracy: 0.7108 - val_accuracy: 0.7108\n",
            "Epoch 3/5\n",
            "7000/7000 [==============================] - 74s 11ms/step - loss: 0.1367 - binary_accuracy: 0.9361 - accuracy: 0.9361 - val_loss: 1.4609 - val_binary_accuracy: 0.7139 - val_accuracy: 0.7139\n",
            "Epoch 4/5\n",
            "7000/7000 [==============================] - 79s 11ms/step - loss: 0.1344 - binary_accuracy: 0.9376 - accuracy: 0.9376 - val_loss: 1.4377 - val_binary_accuracy: 0.7135 - val_accuracy: 0.7135\n",
            "Epoch 5/5\n",
            "7000/7000 [==============================] - 80s 11ms/step - loss: 0.1302 - binary_accuracy: 0.9399 - accuracy: 0.9399 - val_loss: 1.3724 - val_binary_accuracy: 0.7135 - val_accuracy: 0.7135\n",
            "1 loop, best of 5: 6min 21s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFKhkZhGY7Nj",
        "outputId": "5e5e5920-a06a-462b-e4bc-fa4eea607e1e"
      },
      "source": [
        "history"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint2 = ModelCheckpoint(\"rnn_model.hdf5\", monitor='val_accuracy', verbose=1,\n",
            "                              save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
            "history = model2.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),callbacks=[checkpoint2])\n",
            "!pip install \"git+https://github.com/ElenaVillano/sentiment_analysis_tweets.git#egg=nlptweet&subdirectory=src\" --quiet\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import datetime\n",
            "import nltk\n",
            "#import re\n",
            "import timeit\n",
            "import string\n",
            "# Nuestro paquete\n",
            "import nlp\n",
            "\n",
            "\n",
            "#call the nltk downloader\n",
            "nltk.download('punkt')\n",
            "\n",
            "from dateutil import parser\n",
            "\n",
            "# Carga un set de stopwords predefinidas\n",
            "from nltk.corpus import stopwords\n",
            "\n",
            "from nltk.tokenize import sent_tokenize, word_tokenize\n",
            "from nltk.stem import PorterStemmer\n",
            "from nltk.stem import LancasterStemmer\n",
            "# Nombramiento de columnas\n",
            "col_names = ['target', # Polaridad del twet 0=negativo, 2=neutral, 4=positivo\n",
            "             'ids',    # ID tweet\n",
            "             'date',   # Fecha y hora del tweet\n",
            "             'flag',   # QUERY\n",
            "             'user',   # Usuario del tweet\n",
            "             'text']   # Texto del tweety\n",
            "# Requiered to select a file to be imported into colab\n",
            "# Not useful if running locally\n",
            "from google.colab import files\n",
            "uploaded = files.upload()\n",
            "training =  pd.read_csv('smaller_sample_200000.csv',\n",
            "                 encoding='latin-1')\n",
            "test = pd.read_csv('testdata_manual_2009_06_14.csv', names=col_names)\n",
            "print(training.shape)\n",
            "print(test.shape)\n",
            "# Ejemplo\n",
            "training.loc[[4,8,27,41,44,35,48,155]]\n",
            "# primera parte limpieza\n",
            "from nlp.preprocessing import convierte_a_minusculas, reemplazar_urls, reemplazar_usuarios, quitar_hashtag\n",
            "# minusculas\n",
            "training = convierte_a_minusculas(training)\n",
            "test = convierte_a_minusculas(test)\n",
            "# url\n",
            "training['text'] = training['text'].map(lambda s: reemplazar_urls(s))\n",
            "test['text'] = test['text'].map(lambda s: reemplazar_urls(s))\n",
            "# user_mention\n",
            "training['text'] = training['text'].map(lambda s: reemplazar_usuarios(s))\n",
            "test['text'] = test['text'].map(lambda s: reemplazar_usuarios(s))\n",
            "# hashtags\n",
            "training['text'] = training['text'].map(lambda s: quitar_hashtag(s))\n",
            "test['text'] = test['text'].map(lambda s: quitar_hashtag(s))\n",
            "# Ejemplo\n",
            "training.loc[[4,8,27,41,44,35,48,155]]\n",
            "# segunda parte limpieza\n",
            "from nlp.preprocessing import quitar_RT, quitar_caracteres_especiales, quitar_letras_repetidas\n",
            "# retweets\n",
            "training['text'] = training['text'].map(lambda s: quitar_RT(s))\n",
            "test['text'] = test['text'].map(lambda s: quitar_RT(s))\n",
            "# caracteres especiales\n",
            "training['text'] = training['text'].map(lambda s: quitar_caracteres_especiales(s))\n",
            "test['text'] = test['text'].map(lambda s: quitar_caracteres_especiales(s))\n",
            "# letras repetidas\n",
            "training['text'] = training['text'].map(lambda s: quitar_letras_repetidas(s))\n",
            "test['text'] = test['text'].map(lambda s: quitar_letras_repetidas(s))\n",
            "# Ejemplo\n",
            "training.loc[[4,8,27,41,44,35,48,155]]\n",
            "# tercera parte limpieza\n",
            "from nlp.preprocessing import quitar_nonascii, separar_abreviaciones, remove_stopwords, oracion_raiz\n",
            "#nonascii\n",
            "training['text'] = training['text'].map(lambda s: quitar_nonascii(s))\n",
            "test['text'] = test['text'].map(lambda s: quitar_nonascii(s))\n",
            "# abreviaciones\n",
            "training['text'] = training['text'].map(lambda s: separar_abreviaciones(s))\n",
            "test['text'] = test['text'].map(lambda s: separar_abreviaciones(s))\n",
            "# stop words\n",
            "training['text'] = training['text'].map(lambda s: remove_stopwords(s))\n",
            "test['atext'] = test['text'].map(lambda s: remove_stopwords(s))\n",
            "# raiz\n",
            "#training['text'] = training['text'].map(lambda s: oracion_raiz(s))\n",
            "#test['atext'] = test['text'].map(lambda s: oracion_raiz(s))\n",
            "# tercera parte limpieza\n",
            "from nlp.preprocessing import quitar_nonascii, separar_abreviaciones, remove_stopwords, oracion_raiz\n",
            "#nonascii\n",
            "training['text'] = training['text'].map(lambda s: quitar_nonascii(s))\n",
            "test['text'] = test['text'].map(lambda s: quitar_nonascii(s))\n",
            "# abreviaciones\n",
            "training['text'] = training['text'].map(lambda s: separar_abreviaciones(s))\n",
            "test['text'] = test['text'].map(lambda s: separar_abreviaciones(s))\n",
            "# stop words\n",
            "training['text'] = training['text'].map(lambda s: remove_stopwords(s))\n",
            "test['atext'] = test['text'].map(lambda s: remove_stopwords(s))\n",
            "# raiz\n",
            "training['text'] = training['text'].map(lambda s: oracion_raiz(s))\n",
            "test['atext'] = test['text'].map(lambda s: oracion_raiz(s))\n",
            "# Ejemplo\n",
            "training.loc[[4,8,27,41,44,35,48,155]]\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras import regularizers\n",
            "# Entrena un Tokenizer. Consiste en:\n",
            "# Crea un diccionario numerado de las palabras existentes en el corpus, y devuelve\n",
            "# por cada palabra, el número entero de su índice en el diccionario.\n",
            "# En este caso, considera las N palabras más frecuentes.\n",
            "# oov_token = constante asiganda para palabras fuera del vocabulario (NOT USED HERE)\n",
            "data_set_1 = training\n",
            "data_set_1.shape\n",
            "\n",
            "max_words = 5000\n",
            "max_len = 200\n",
            "\n",
            "tokenizer = Tokenizer(num_words=max_words)\n",
            "tokenizer.fit_on_texts(data_set_1.text)\n",
            "sequences = tokenizer.texts_to_sequences(data_set_1.text)\n",
            "tweets = pad_sequences(sequences, maxlen=max_len)\n",
            "print(tweets)\n",
            "from sklearn.model_selection import train_test_split\n",
            "X_train, X_test, y_train, y_test = train_test_split(tweets, data_set_1.target.values, test_size=0.3, random_state=23042021)\n",
            "print(\"X_train\", X_train.shape)\n",
            "print(\"y_train\", y_train.shape)\n",
            "print(\"X_test\", X_test.shape)\n",
            "print(\"y_test\", y_test.shape)\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
            "from tensorflow.keras.regularizers import l1\n",
            "\n",
            "# Crea una red con layers Embedding, LSTM, Dense\n",
            "model = Sequential()\n",
            "model.add(Embedding(input_length=128, input_dim=1000, output_dim=64, trainable=True))\n",
            "model.add(LSTM(units=64))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=512, activation='relu'))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=256, activation='relu'))\n",
            "model.add(Dense(units=82, activation=None))\n",
            "\n",
            "model.summary()\n",
            "#from keras.models import Sequential\n",
            "#from keras import regularizers\n",
            "#from keras import layers\n",
            "#from keras import backend as K\n",
            "#from keras.callbacks import ModelCheckpoint\n",
            "\n",
            "\n",
            "#model2 = Sequential()\n",
            "#model2.add(layers.Embedding(max_words, 128))\n",
            "#model2.add(layers.LSTM(64,dropout=0.5))\n",
            "#model2.add(layers.Dense(16, activation='relu'))\n",
            "#model2.add(layers.Dense(8, activation='relu'))\n",
            "#model2.add(layers.Dense(1,activation='sigmoid'))\n",
            "#model2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
            "\n",
            "#model2.summary()\n",
            "# Compile\n",
            "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
            "              optimizer='adam', metrics=[tf.metrics.BinaryAccuracy()])\n",
            "# Train it\n",
            "history = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))\n",
            "# Compile\n",
            "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n",
            "              optimizer='adam', metrics=[tf.metrics.BinaryAccuracy()])\n",
            "# Train it\n",
            "history = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))\n",
            "#import tensorflow as tf\n",
            "#from tensorflow.keras.models import Sequential\n",
            "#from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
            "#from tensorflow.keras.regularizers import l1\n",
            "\n",
            "# Crea una red con layers Embedding, LSTM, Dense\n",
            "#model = Sequential()\n",
            "#model.add(Embedding(input_length=128, input_dim=1000, output_dim=64, trainable=True))\n",
            "#model.add(LSTM(units=64))\n",
            "#model.add(Dropout(0.5))\n",
            "#model.add(Dense(units=512, activation='relu'))\n",
            "#model.add(Dropout(0.5))\n",
            "#model.add(Dense(units=256, activation='relu'))\n",
            "#model.add(Dense(units=82, activation=None))\n",
            "\n",
            "#model.summary()\n",
            "# Compile\n",
            "#model.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n",
            "#              optimizer='adam', metrics=[tf.metrics.BinaryAccuracy()])\n",
            "# Train it\n",
            "#history = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))\n",
            "from keras.models import Sequential\n",
            "from keras import regularizers\n",
            "from keras import layers\n",
            "from keras import backend as K\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "\n",
            "\n",
            "model2 = Sequential()\n",
            "model2.add(layers.Embedding(max_words, 128))\n",
            "model2.add(layers.LSTM(64))\n",
            "model2.add(Dropout(0.5))\n",
            "model2.add(layers.Dense(512, activation='relu'))\n",
            "model2.add(Dropout(0.5))\n",
            "model2.add(layers.Dense(256, activation='relu'))\n",
            "model2.add(layers.Dense(86,activation='sigmoid'))\n",
            "model2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
            "\n",
            "model2.summary()\n",
            "checkpoint2 = ModelCheckpoint(\"rnn_model.hdf5\", monitor='val_accuracy', verbose=1,\n",
            "                              save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
            "history = model2.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),callbacks=[checkpoint2])\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras import regularizers\n",
            "# Entrena un Tokenizer. Consiste en:\n",
            "# Crea un diccionario numerado de las palabras existentes en el corpus, y devuelve\n",
            "# por cada palabra, el número entero de su índice en el diccionario.\n",
            "# En este caso, considera las N palabras más frecuentes.\n",
            "# oov_token = constante asiganda para palabras fuera del vocabulario (NOT USED HERE)\n",
            "data_set_1 = training\n",
            "data_set_1.shape\n",
            "\n",
            "max_words = 5000\n",
            "max_len = 200\n",
            "\n",
            "tokenizer = Tokenizer(num_words=max_words)\n",
            "tokenizer.fit_on_texts(data_set_1.text)\n",
            "sequences = tokenizer.texts_to_sequences(data_set_1.text)\n",
            "tweets = pad_sequences(sequences, maxlen=max_len)\n",
            "print(tweets)\n",
            "X_train\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras import regularizers\n",
            "# Entrena un Tokenizer. Consiste en:\n",
            "# Crea un diccionario numerado de las palabras existentes en el corpus, y devuelve\n",
            "# por cada palabra, el número entero de su índice en el diccionario.\n",
            "# En este caso, considera las N palabras más frecuentes.\n",
            "# oov_token = constante asiganda para palabras fuera del vocabulario (NOT USED HERE)\n",
            "data_set_1 = training\n",
            "data_set_1.shape\n",
            "\n",
            "max_words = 5000\n",
            "max_len = 200\n",
            "\n",
            "tokenizer = Tokenizer(num_words=max_words)\n",
            "tokenizer.fit_on_texts(data_set_1.text)\n",
            "sequences = tokenizer.texts_to_sequences(data_set_1.text)\n",
            "tweets = pad_sequences(sequences, maxlen=max_len)\n",
            "print(tweets)\n",
            "from sklearn.model_selection import train_test_split\n",
            "X_train, X_test, y_train, y_test = train_test_split(tweets, data_set_1.target.values, test_size=0.3, random_state=23042021)\n",
            "print(\"X_train\", X_train.shape)\n",
            "print(\"y_train\", y_train.shape)\n",
            "print(\"X_test\", X_test.shape)\n",
            "print(\"y_test\", y_test.shape)\n",
            "from keras.models import Sequential\n",
            "from keras import regularizers\n",
            "from keras import layers\n",
            "from keras import backend as K\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "\n",
            "\n",
            "model2 = Sequential()\n",
            "model2.add(layers.Embedding(max_words, 128))\n",
            "model2.add(layers.LSTM(64))\n",
            "model2.add(Dropout(0.5))\n",
            "model2.add(layers.Dense(512, activation='relu'))\n",
            "model2.add(Dropout(0.5))\n",
            "model2.add(layers.Dense(256, activation='relu'))\n",
            "model2.add(layers.Dense(86,activation='sigmoid'))\n",
            "model2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
            "\n",
            "model2.summary()\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
            "from tensorflow.keras.regularizers import l1\n",
            "# Crea una red con layers Embedding, LSTM, Dense\n",
            "model = Sequential()\n",
            "model.add(Embedding(input_length=128, input_dim=1000, output_dim=64, trainable=True))\n",
            "model.add(LSTM(units=64))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=512, activation='relu'))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=256, activation='relu'))\n",
            "model.add(Dense(units=82, activation=None))\n",
            "\n",
            "model.summary()\n",
            "# Compile\n",
            "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(), 'accuracy'])\n",
            "%%timeit\n",
            "\n",
            "# Train it\n",
            "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n",
            "# Crea una red con layers Embedding, LSTM, Dense\n",
            "model = Sequential()\n",
            "model.add(Embedding(input_length=128, input_dim=max_words, output_dim=64, trainable=True))\n",
            "model.add(LSTM(units=64))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=512, activation='relu'))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=256, activation='relu'))\n",
            "model.add(Dense(units=82, activation=None))\n",
            "\n",
            "model.summary()\n",
            "# Compile\n",
            "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(), 'accuracy'])\n",
            "%%timeit\n",
            "\n",
            "# Train it\n",
            "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n",
            "# Crea una red con layers Embedding, LSTM, Dense\n",
            "model = Sequential()\n",
            "model.add(Embedding(input_length=128, input_dim=max_words, output_dim=64, trainable=True))\n",
            "model.add(LSTM(units=64))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=256, activation='relu'))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=82, activation='relu'))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=1, activation=None))\n",
            "\n",
            "model.summary()\n",
            "# Compile\n",
            "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(), 'accuracy'])\n",
            "%%timeit\n",
            "\n",
            "# Train it\n",
            "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n",
            "y_train\n",
            "y_train = np.where(y_train==4,1,y_train)\n",
            "y_train\n",
            "y_train = np.where(y_train==4,1,y_train)\n",
            "y_test = np.where(y_test==4,1,y_test)\n",
            "y_train\n",
            "y_test\n",
            "%%timeit\n",
            "\n",
            "# Train it\n",
            "history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test,y_test))Ç\n",
            "%%timeit\n",
            "\n",
            "# Train it\n",
            "history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test,y_test))\n",
            "# Crea una red con layers Embedding, LSTM, Dense\n",
            "model = Sequential()\n",
            "model.add(Embedding(input_length=128, input_dim=max_words, output_dim=64, trainable=True))\n",
            "model.add(LSTM(units=64))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=82, activation='relu'))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=1, activation=None))\n",
            "\n",
            "model.summary()\n",
            "# Compile\n",
            "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(), 'accuracy'])\n",
            "%%timeit\n",
            "\n",
            "# Train it\n",
            "history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test,y_test))\n",
            "# Entrena un Tokenizer. Consiste en:\n",
            "# Crea un diccionario numerado de las palabras existentes en el corpus, y devuelve\n",
            "# por cada palabra, el número entero de su índice en el diccionario.\n",
            "# En este caso, considera las N palabras más frecuentes.\n",
            "# oov_token = constante asiganda para palabras fuera del vocabulario (NOT USED HERE)\n",
            "data_set_1 = training\n",
            "data_set_1.shape\n",
            "# Entrena un Tokenizer. Consiste en:\n",
            "# Crea un diccionario numerado de las palabras existentes en el corpus, y devuelve\n",
            "# por cada palabra, el número entero de su índice en el diccionario.\n",
            "# En este caso, considera las N palabras más frecuentes.\n",
            "# oov_token = constante asiganda para palabras fuera del vocabulario (NOT USED HERE)\n",
            "data_set_1 = training\n",
            "data_set_1.shape\n",
            "data_set_1\n",
            "# Entrena un Tokenizer. Consiste en:\n",
            "# Crea un diccionario numerado de las palabras existentes en el corpus, y devuelve\n",
            "# por cada palabra, el número entero de su índice en el diccionario.\n",
            "# En este caso, considera las N palabras más frecuentes.\n",
            "# oov_token = constante asiganda para palabras fuera del vocabulario (NOT USED HERE)\n",
            "data_set_1 = training\n",
            "data_set_1.shape\n",
            "data_set_1.text\n",
            "max_words = 5000\n",
            "max_len = 200\n",
            "\n",
            "tokenizer = Tokenizer(num_words=max_words)\n",
            "tokenizer.fit_on_texts(data_set_1.text)\n",
            "\n",
            "print(\"Number of words in the dictionary\", len(tokenizer.word_index))\n",
            "print(list(tokenizer.word_index.items())[:10])\n",
            "sorted(tokenizer.word_counts.values())[-30:]\n",
            "#Tokenizer?\n",
            "# Convierte palabras en enteros\n",
            "train_sequences = tokenizer.texts_to_sequences(data_set_1.text)\n",
            "lengths = []\n",
            "for seq in train_sequences:\n",
            "  lengths.append(len(seq))\n",
            "\n",
            "print(\"Shortest article with\", np.array(lengths).min(), \"words.\")\n",
            "print(\"Longest article with\", np.array(lengths).max(), \"words.\")\n",
            "print(f\"Average length: {np.array(lengths).mean():.2f} +- {np.array(lengths).std():.2f} words.\")\n",
            "print('\\n')\n",
            "\n",
            "for seq in train_sequences[:10]:\n",
            "  print(seq)\n",
            "# Convierte palabras en enteros\n",
            "train_sequences = tokenizer.texts_to_sequences(data_set_1.text)\n",
            "lengths = []\n",
            "for seq in train_sequences:\n",
            "  lengths.append(len(seq))\n",
            "\n",
            "print(\"Tweet más pequeño\", np.array(lengths).min(), \"words.\")\n",
            "print(\"Tweet más grande\", np.array(lengths).max(), \"words.\")\n",
            "print(f\"Longitud promedio: {np.array(lengths).mean():.2f} +- {np.array(lengths).std():.2f} words.\")\n",
            "print('\\n')\n",
            "\n",
            "for seq in train_sequences[:10]:\n",
            "  print(seq)\n",
            "plt.hist(lengths, 50)\n",
            "plt.show()\n",
            "plt.hist(lengths)\n",
            "plt.show()\n",
            "plt.hist(lengths,20)\n",
            "plt.show()\n",
            "sorted(tokenizer.word_counts.values())[:10]\n",
            "#Tokenizer?\n",
            "sequences\n",
            "tweets\n",
            "data_set_1.target.values\n",
            "from sklearn.model_selection import train_test_split\n",
            "X_train, X_test, y_train, y_test = train_test_split(tweets, data_set_1.target.values, test_size=0.3, random_state=23042021)\n",
            "print(\"X_train\", X_train.shape)\n",
            "print(\"y_train\", y_train.shape)\n",
            "print(\"X_test\", X_test.shape)\n",
            "print(\"y_test\", y_test.shape)\n",
            "y_train = np.where(y_train==4,1,y_train)\n",
            "y_test = np.where(y_test==4,1,y_test)\n",
            "y_train\n",
            "y_test\n",
            "training.text\n",
            "max_words = 5000\n",
            "max_len = 128\n",
            "\n",
            "tokenizer = Tokenizer(num_words=max_words)\n",
            "tokenizer.fit_on_texts(data_set_1.text)\n",
            "\n",
            "print(\"Number of words in the dictionary\", len(tokenizer.word_index))\n",
            "print(list(tokenizer.word_index.items())[:10])\n",
            "# Convierte palabras en enteros\n",
            "train_sequences = tokenizer.texts_to_sequences(data_set_1.text)\n",
            "lengths = []\n",
            "for seq in train_sequences:\n",
            "  lengths.append(len(seq))\n",
            "\n",
            "print(\"Tweet más pequeño\", np.array(lengths).min(), \"words.\")\n",
            "print(\"Tweet más grande\", np.array(lengths).max(), \"words.\")\n",
            "print(f\"Longitud promedio: {np.array(lengths).mean():.2f} +- {np.array(lengths).std():.2f} words.\")\n",
            "print('\\n')\n",
            "\n",
            "for seq in train_sequences[:10]:\n",
            "  print(seq)\n",
            "sequences = tokenizer.texts_to_sequences(data_set_1.text)\n",
            "tweets = pad_sequences(sequences, maxlen=max_len)\n",
            "print(tweets)\n",
            "tweets\n",
            "tweets.shape\n",
            "data_set_1.target.values\n",
            "data_set_1.target.shape\n",
            "from sklearn.model_selection import train_test_split\n",
            "X_train, X_test, y_train, y_test = train_test_split(tweets, data_set_1.target.values, test_size=0.3, random_state=23042021)\n",
            "print(\"X_train\", X_train.shape)\n",
            "print(\"y_train\", y_train.shape)\n",
            "print(\"X_test\", X_test.shape)\n",
            "print(\"y_test\", y_test.shape)\n",
            "y_train = np.where(y_train==4,1,y_train)\n",
            "y_test = np.where(y_test==4,1,y_test)\n",
            "y_train\n",
            "y_test\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
            "from tensorflow.keras.regularizers import l1\n",
            "# Crea una red con layers Embedding, LSTM, Dense\n",
            "model = Sequential()\n",
            "model.add(Embedding(input_length=128, input_dim=max_words, output_dim=64, trainable=True))\n",
            "model.add(LSTM(units=64))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=82, activation='relu'))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(units=1, activation=None))\n",
            "\n",
            "model.summary()\n",
            "# Compile\n",
            "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(), 'accuracy'])\n",
            "%%timeit\n",
            "\n",
            "# Train it\n",
            "history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_split=0.2)\n",
            "!pip install \"git+https://github.com/ElenaVillano/sentiment_analysis_tweets.git#egg=nlptweet&subdirectory=src\" --quiet\n",
            "# Plot loss\n",
            "plt.figure(figsize=(20, 4))\n",
            "\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.title('Loss')\n",
            "plt.plot(history.history['loss'], label='Training', linewidth=2)\n",
            "plt.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Loss')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.subplot(1, 2, 2)\n",
            "plt.title('Accuracy')\n",
            "plt.plot(history.history['binary_accuracy'], label='Training', linewidth=2)\n",
            "plt.plot(history.history['val_binary_accuracy'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.show()\n",
            "# Plot loss\n",
            "plt.figure(figsize=(20, 4))\n",
            "\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.title('Loss')\n",
            "plt.plot(history.history['loss'], label='Training', linewidth=2)\n",
            "plt.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Loss')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.subplot(1, 2, 2)\n",
            "plt.title('Accuracy')\n",
            "plt.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
            "plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.show()\n",
            "# Plot loss\n",
            "plt.figure(figsize=(20, 4))\n",
            "\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.title('Loss')\n",
            "plt.plot(history['loss'], label='Training', linewidth=2)\n",
            "plt.plot(history['val_loss'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Loss')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.subplot(1, 2, 2)\n",
            "plt.title('Accuracy')\n",
            "plt.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
            "plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.show()\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import datetime\n",
            "import nltk\n",
            "#import re\n",
            "import timeit\n",
            "import string\n",
            "# Nuestro paquete\n",
            "import nlp\n",
            "\n",
            "\n",
            "#call the nltk downloader\n",
            "nltk.download('punkt')\n",
            "\n",
            "from dateutil import parser\n",
            "\n",
            "# Carga un set de stopwords predefinidas\n",
            "from nltk.corpus import stopwords\n",
            "\n",
            "from nltk.tokenize import sent_tokenize, word_tokenize\n",
            "from nltk.stem import PorterStemmer\n",
            "from nltk.stem import LancasterStemmer\n",
            "# Plot loss\n",
            "plt.figure(figsize=(20, 4))\n",
            "\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.title('Loss')\n",
            "plt.plot(hihistory.historystory['loss'], label='Training', linewidth=2)\n",
            "plt.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Loss')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.subplot(1, 2, 2)\n",
            "plt.title('Accuracy')\n",
            "plt.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
            "plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.show()\n",
            "%history\n",
            "history\n",
            "history\n",
            "history.history\n",
            "# Plot loss\n",
            "plt.figure(figsize=(20, 4))\n",
            "\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.title('Loss')\n",
            "plt.plot(history.history['loss'], label='Training', linewidth=2)\n",
            "plt.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Loss')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.subplot(1, 2, 2)\n",
            "plt.title('Accuracy')\n",
            "plt.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
            "plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
            "plt.legend()\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.grid(True)\n",
            "\n",
            "plt.show()\n",
            "history\n",
            "history\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "nZwvQmAROlem",
        "outputId": "4b76be00-98e7-47a3-9021-b25d6afcd7e6"
      },
      "source": [
        "# Plot loss\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], label='Training', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-48a02081cd6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAEICAYAAAB1SQ8uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQBklEQVR4nO3df4zkdX3H8ddbjh+poja9M1E4AeOhXmxT6ZbS2FRaaAOkOf6wMVxKrYZIYotpKzHFaNFi/6gltcYEq2dqrDaC6B/2UjE0tRiM8QxLqVQgmBNRDmw4FYkNVUDf/WOGZl3v2PGY3f305vFINpnvzGdn3pdP7ngy893vVncHAGAkT9vsAQAAVhMoAMBwBAoAMByBAgAMR6AAAMMRKADAcAQKADAcgQIcsaq6t6rO3ew5gKOPQAEAhiNQgLmqquOr6t1V9cD0691Vdfz0sa1V9c9V9d2q+k5Vfa6qnjZ97M+q6v6q+l5V3V1V52zunwTYTFs2ewDgqPOWJGcl+cUkneSfkrw1yZ8nuTzJgSTbpmvPStJV9aIklyX55e5+oKpOTXLMxo4NjMQ7KMC8/V6Sq7r7we4+mOQvkvz+9LHHkjw3ySnd/Vh3f64nvxDsh0mOT7Kzqo7t7nu7+6ubMj0wBIECzNvzknx9xfHXp/clydVJ9if5l6q6p6quSJLu3p/kT5K8PcmDVXVdVT0vwMISKMC8PZDklBXHz5/el+7+Xndf3t0vSLIryRufONekuz/a3b82/d5O8s6NHRsYiUABnqpjq+qEJ76SXJvkrVW1raq2JrkyyT8mSVX9TlW9sKoqycOZfLTzo6p6UVX95vRk2u8n+Z8kP9qcPw4wAoECPFU3ZBIUT3ydkGQ5ye1J/jPJvyf5y+naHUn+Ncl/J/lCkvd2902ZnH/yV0m+leS/kjwnyZs37o8AjKYm56cBAIzDOygAwHDWDJSq+mBVPVhVXz7M41VV76mq/VV1e1WdMf8xAYBFMss7KB9Kct6TPH5+Jp8r70hyaZK/e+pjAQCLbM1A6e6bk3znSZZcmOTDPbEvybOr6rnzGhAAWDzzuNT9SUnuW3F8YHrfN1cvrKpLM3mXJU9/+tN/6cUvfvEcXh4AGNGtt976re7etvbKn7Shv4unu/ck2ZMkS0tLvby8vJEvDwBsoKr6+tqrDm0eP8Vzf5LtK45Pnt4HAHBE5hEoe5O8evrTPGclebi7f+LjHQCAWa35EU9VXZvk7CRbq+pAkrclOTZJuvt9mVxF8oJMfgHYI0leu17DAgCLYc1A6e7dazzeSf5obhMBAAvPlWQBgOEIFABgOAIFABiOQAEAhiNQAIDhCBQAYDgCBQAYjkABAIYjUACA4QgUAGA4AgUAGI5AAQCGI1AAgOEIFABgOAIFABiOQAEAhiNQAIDhCBQAYDgCBQAYjkABAIYjUACA4QgUAGA4AgUAGI5AAQCGI1AAgOEIFABgOAIFABiOQAEAhiNQAIDhCBQAYDgCBQAYjkABAIYjUACA4QgUAGA4AgUAGM5MgVJV51XV3VW1v6quOMTjz6+qm6rqtqq6vaoumP+oAMCiWDNQquqYJNckOT/JziS7q2rnqmVvTXJ9d78syUVJ3jvvQQGAxTHLOyhnJtnf3fd096NJrkty4ao1neSZ09vPSvLA/EYEABbNLIFyUpL7VhwfmN630tuTXFxVB5LckOQNh3qiqrq0qparavngwYNHMC4AsAjmdZLs7iQf6u6Tk1yQ5CNV9RPP3d17unupu5e2bds2p5cGAI42swTK/Um2rzg+eXrfSpckuT5JuvsLSU5IsnUeAwIAi2eWQLklyY6qOq2qjsvkJNi9q9Z8I8k5SVJVL8kkUHyGAwAckTUDpbsfT3JZkhuT3JXJT+vcUVVXVdWu6bLLk7yuqr6U5Nokr+nuXq+hAYCj25ZZFnX3DZmc/LryvitX3L4zycvnOxoAsKhcSRYAGI5AAQCGI1AAgOEIFABgOAIFABiOQAEAhiNQAIDhCBQAYDgCBQAYjkABAIYjUACA4QgUAGA4AgUAGI5AAQCGI1AAgOEIFABgOAIFABiOQAEAhiNQAIDhCBQAYDgCBQAYjkABAIYjUACA4QgUAGA4AgUAGI5AAQCGI1AAgOEIFABgOAIFABiOQAEAhiNQAIDhCBQAYDgCBQAYjkABAIYzU6BU1XlVdXdV7a+qKw6z5lVVdWdV3VFVH53vmADAItmy1oKqOibJNUl+K8mBJLdU1d7uvnPFmh1J3pzk5d39UFU9Z70GBgCOfrO8g3Jmkv3dfU93P5rkuiQXrlrzuiTXdPdDSdLdD853TABgkcwSKCcluW/F8YHpfSudnuT0qvp8Ve2rqvMO9URVdWlVLVfV8sGDB49sYgDgqDevk2S3JNmR5Owku5N8oKqevXpRd+/p7qXuXtq2bducXhoAONrMEij3J9m+4vjk6X0rHUiyt7sf6+6vJflKJsECAPBTmyVQbkmyo6pOq6rjklyUZO+qNZ/M5N2TVNXWTD7yuWeOcwIAC2TNQOnux5NcluTGJHclub6776iqq6pq13TZjUm+XVV3JrkpyZu6+9vrNTQAcHSr7t6UF15aWurl5eVNeW0AYP1V1a3dvXQk3+tKsgDAcAQKADAcgQIADEegAADDESgAwHAECgAwHIECAAxHoAAAwxEoAMBwBAoAMByBAgAMR6AAAMMRKADAcAQKADAcgQIADEegAADDESgAwHAECgAwHIECAAxHoAAAwxEoAMBwBAoAMByBAgAMR6AAAMMRKADAcAQKADAcgQIADEegAADDESgAwHAECgAwHIECAAxHoAAAwxEoAMBwBAoAMByBAgAMZ6ZAqarzquruqtpfVVc8ybpXVlVX1dL8RgQAFs2agVJVxyS5Jsn5SXYm2V1VOw+x7sQkf5zki/MeEgBYLLO8g3Jmkv3dfU93P5rkuiQXHmLdO5K8M8n35zgfALCAZgmUk5Lct+L4wPS+/1NVZyTZ3t2ferInqqpLq2q5qpYPHjz4Uw8LACyGp3ySbFU9Lcm7kly+1tru3tPdS929tG3btqf60gDAUWqWQLk/yfYVxydP73vCiUlemuSzVXVvkrOS7HWiLABwpGYJlFuS7Kiq06rquCQXJdn7xIPd/XB3b+3uU7v71CT7kuzq7uV1mRgAOOqtGSjd/XiSy5LcmOSuJNd39x1VdVVV7VrvAQGAxbNllkXdfUOSG1bdd+Vh1p791McCABaZK8kCAMMRKADAcAQKADAcgQIADEegAADDESgAwHAECgAwHIECAAxHoAAAwxEoAMBwBAoAMByBAgAMR6AAAMMRKADAcAQKADAcgQIADEegAADDESgAwHAECgAwHIECAAxHoAAAwxEoAMBwBAoAMByBAgAMR6AAAMMRKADAcAQKADAcgQIADEegAADDESgAwHAECgAwHIECAAxHoAAAwxEoAMBwZgqUqjqvqu6uqv1VdcUhHn9jVd1ZVbdX1Weq6pT5jwoALIo1A6WqjklyTZLzk+xMsruqdq5adluSpe7+hSSfSPLX8x4UAFgcs7yDcmaS/d19T3c/muS6JBeuXNDdN3X3I9PDfUlOnu+YAMAimSVQTkpy34rjA9P7DueSJJ8+1ANVdWlVLVfV8sGDB2efEgBYKHM9SbaqLk6ylOTqQz3e3Xu6e6m7l7Zt2zbPlwYAjiJbZlhzf5LtK45Pnt73Y6rq3CRvSfKK7v7BfMYDABbRLO+g3JJkR1WdVlXHJbkoyd6VC6rqZUnen2RXdz84/zEBgEWyZqB09+NJLktyY5K7klzf3XdU1VVVtWu67Ookz0jy8ar6j6rae5inAwBY0ywf8aS7b0hyw6r7rlxx+9w5zwUALDBXkgUAhiNQAIDhCBQAYDgCBQAYjkABAIYjUACA4QgUAGA4AgUAGI5AAQCGI1AAgOEIFABgOAIFABiOQAEAhiNQAIDhCBQAYDgCBQAYjkABAIYjUACA4QgUAGA4AgUAGI5AAQCGI1AAgOEIFABgOAIFABiOQAEAhiNQAIDhCBQAYDgCBQAYjkABAIYjUACA4QgUAGA4AgUAGI5AAQCGI1AAgOEIFABgODMFSlWdV1V3V9X+qrriEI8fX1Ufmz7+xao6dd6DAgCLY81AqapjklyT5PwkO5Psrqqdq5ZdkuSh7n5hkr9N8s55DwoALI5Z3kE5M8n+7r6nux9Ncl2SC1etuTDJP0xvfyLJOVVV8xsTAFgkW2ZYc1KS+1YcH0jyK4db092PV9XDSX4uybdWLqqqS5NcOj38QVV9+UiGZl1tzap9Y9PZk/HYkzHZl/G86Ei/cZZAmZvu3pNkT5JU1XJ3L23k67M2+zIeezIeezIm+zKeqlo+0u+d5SOe+5NsX3F88vS+Q66pqi1JnpXk20c6FACw2GYJlFuS7Kiq06rquCQXJdm7as3eJH8wvf27Sf6tu3t+YwIAi2TNj3im55RcluTGJMck+WB331FVVyVZ7u69Sf4+yUeqan+S72QSMWvZ8xTmZv3Yl/HYk/HYkzHZl/Ec8Z6UNzoAgNG4kiwAMByBAgAMZ90DxWXyxzPDnryxqu6sqtur6jNVdcpmzLlo1tqXFeteWVVdVX6ccp3NsidV9arp35c7quqjGz3jIprh37DnV9VNVXXb9N+xCzZjzkVSVR+sqgcPd32zmnjPdM9ur6oz1nzS7l63r0xOqv1qkhckOS7Jl5LsXLXmD5O8b3r7oiQfW8+ZFv1rxj35jSQ/M739ensyxr5M152Y5OYk+5IsbfbcR/PXjH9XdiS5LcnPTo+fs9lzH+1fM+7LniSvn97emeTezZ77aP9K8utJzkjy5cM8fkGSTyepJGcl+eJaz7ne76C4TP541tyT7r6pux+ZHu7L5No3rK9Z/q4kyTsy+V1X39/I4RbULHvyuiTXdPdDSdLdD27wjItoln3pJM+c3n5Wkgc2cL6F1N03Z/JTvIdzYZIP98S+JM+uquc+2XOud6Ac6jL5Jx1uTXc/nuSJy+SzPmbZk5UuyaR6WV9r7sv0LdHt3f2pjRxsgc3yd+X0JKdX1eeral9Vnbdh0y2uWfbl7UkurqoDSW5I8oaNGY0n8dP+t2djL3XP/y9VdXGSpSSv2OxZFl1VPS3Ju5K8ZpNH4cdtyeRjnrMzeafx5qr6+e7+7qZOxe4kH+ruv6mqX83kOl0v7e4fbfZgzG6930FxmfzxzLInqapzk7wlya7u/sEGzbbI1tqXE5O8NMlnq+reTD7D3etE2XU1y9+VA0n2dvdj3f21JF/JJFhYP7PsyyVJrk+S7v5CkhMy+UWCbJ6Z/tuz0noHisvkj2fNPamqlyV5fyZx4jP1jfGk+9LdD3f31u4+tbtPzeTcoF3dfcS/iIs1zfLv1yczefckVbU1k4987tnIIRfQLPvyjSTnJElVvSSTQDm4oVOy2t4kr57+NM9ZSR7u7m8+2Tes60c8vX6XyecIzbgnVyd5RpKPT89X/kZ379q0oRfAjPvCBppxT25M8ttVdWeSHyZ5U3d7B3gdzbgvlyf5QFX9aSYnzL7G//iur6q6NpNY3zo99+dtSY5Nku5+XybnAl2QZH+SR5K8ds3ntGcAwGhcSRYAGI5AAQCGI1AAgOEIFABgOAIFABiOQAEAhiNQAIDh/C807NQjqO2hHwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34NhUm_3Xupm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}